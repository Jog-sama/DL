<html lang="" dir="ltr">

<head>
  <meta charset="utf-8">
  <title>DL PRACS</title>
</head>

<body>

    <pre>

        import numpy as np
        import matplotlib.pyplot as plt
        
        
        
        X = [0.3,0.5,0.7,0.4]
        Y = [0.2,0.6,0.9,0.3]
        
        
        
        def sigmoid(w,x,b):
          return (1.0/(1.0+np.exp(-(w*x + b))))
        
        def error(w,b,X,Y):
          errors = 0.0
          for x,y in zip(X,Y):
            y_pred = sigmoid(w,x,b)
            errors += 0.5*(y_pred - y)**2
        
          return errors
        
        def grad_w(w,b,X,Y):
          y_pred = sigmoid(w,X,b)
          return ((y_pred - Y)*y_pred*(1-y_pred)*X)
        
        
        def grad_b(w,b,X,Y):
          y_pred = sigmoid(w,X,b)
          return ((y_pred - Y)*y_pred*(1-y_pred))
        
        
        def plts(errors:list):
          index = [i for i in range(len(errors))]
          plt.plot(index , errors)
          plt.xlabel("epoch")
          plt.ylabel("Error")
          plt.show()


          
        #gradient descent vanilla 

        def gradient_decent():
          w, b, lr, epoch = 2,-2,1,100
          errors = []
          print(X,Y)
          print(f"Error before updating weights: {error(w,b,X,Y)}")
        
          for i in range(epoch):
            errors.append(error(w,b,X,Y))
            dw,db = 0,0
            for x,y in zip(X,Y):
              # print(x,y)
              dw+=grad_w(w,b,x,y)
              db+=grad_b(w,b,x,y)
        
            w = w - lr*dw
            b = b - lr*db
        
          print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")
          return errors
        
        
        list_error = gradient_decent()
        
        plts(list_error)


        # stocastic gradient decent
        import random
        
        def stocastic_gradient_decent():
          w, b, lr, epoch = 2,-2,1,100
          errors = []
          print(X,Y)
          print(f"Error before updating weights: {error(w,b,X,Y)}")
        
          for i in range(epoch):
            errors.append(error(w,b,X,Y))
            dw,db = 0,0
            # print(random.randint(0,3))
            ran = random.randint(0,3)
            # for x,y in zip(X,Y):
              # print(x,y)
            dw+=grad_w(w,b,X[ran],Y[ran])
            db+=grad_b(w,b,X[ran],Y[ran])
        
            w = w - lr*dw
            b = b - lr*db
        
          print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")
          return errors
        
        
        list_error = stocastic_gradient_decent()
        
        plts(list_error)

        
        # mini batch gradient decent
        import random
        
        def batch_gradient_decent():
          w, b, lr, epoch = 2,-2,1,100
          errors = []
        
          print(X,Y)
          print(f"Error before updating weights: {error(w,b,X,Y)}")
          batch_size = 2
        
          for i in range(epoch):
            errors.append(error(w,b,X,Y))
            dw,db = 0,0
            X_b= X[0:batch_size]
            Y_b= Y[0:batch_size]
            for x,y in zip(X_b,Y_b):
              # print(x,y)
              dw+=grad_w(w,b,x,y)
              db+=grad_b(w,b,x,y)
        
            w = w - lr*dw
            b = b - lr*db
        
          print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")
          return errors
        
        
        list_error = batch_gradient_decent()
        
        plts(list_error)


        def momentum_gradient_decent():
        w,b,lr,epochs = 2,-2,0.1,100
        errors = []
        print(X,Y)
        prev_v_w, prev_v_b , gamma = 0,0,0.9
        print(f"Error before updating weights: {error(w,b,X,Y)}")
        for i in range(epochs):
          dw,db = 0,0
          errors.append(error(w,b,X,Y))
          for x,y in zip(X,Y):
            dw+=grad_w(w,b,x,y)
            db+=grad_b(w,b,x,y)
          v_w = gamma * prev_v_w + lr*dw
          v_b= gamma * prev_v_b + lr*db
      
          w = w - v_w
          b = b - v_b
          prev_v_w = v_w
          prev_v_b = v_b
      
        print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")
      
        return errors
      
      
      list_error = momentum_gradient_decent()
      
      plts(list_error)



    def nestrov_gradient_decent():
    w,b,lr,epochs = 2,-2,0.1,100
    errors = []
    print(X,Y)
    prev_v_w, prev_v_b , gamma = 0,0,0.9
    print(f"Error before updating weights: {error(w,b,X,Y)}")
    for i in range(epochs):
        dw,db = 0,0
        errors.append(error(w,b,X,Y))
        for x,y in zip(X,Y):
        dw+=grad_w(w - gamma * prev_v_w,b - gamma * prev_v_b,x,y)
        db+=grad_b(w - gamma * prev_v_w,b - gamma * prev_v_b,x,y)
        v_w = gamma * prev_v_w + lr*dw
        v_b= gamma * prev_v_b + lr*db

        w = w - v_w
        b = b - v_b
        prev_v_w = v_w
        prev_v_b = v_b

    print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")

    return errors


    list_error = nestrov_gradient_decent()

    plts(list_error)

    def RMS():
      w,b,lr,epochs= 2,-2,0.1,100
      v_w,v_b, eps  , beta=0,0 ,1e-8 , 0.9
      errors = []
      print(X,Y)
      print(f"Error before updating weights: {error(w,b,X,Y)}")
      for i in range(epochs):
        dw,db = 0,0
        errors.append(error(w,b,X,Y))
        for x,y in zip(X,Y):
          dw+=grad_w(w,b,x,y)
          db+=grad_b(w,b,x,y)
    
        v_w = beta*v_w + (1-beta)*dw**2
        v_b = beta*v_b + (1-beta)*db**2
    
        w = w - (lr/(np.sqrt(v_w) + eps))*dw
        b = b - (lr/(np.sqrt(v_b) + eps))*db
    
    
      print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")
    
      return errors
    
    list_error = RMS()
    
    plts(list_error)


def adagrad():
  w,b,lr,epochs= 2,-2,0.1,100
  v_w,v_b, eps =0,0 ,1e-8
  errors = []
  print(X,Y)
  print(f"Error before updating weights: {error(w,b,X,Y)}")
  for i in range(epochs):
    dw,db = 0,0
    errors.append(error(w,b,X,Y))
    for x,y in zip(X,Y):
      dw+=grad_w(w,b,x,y)
      db+=grad_b(w,b,x,y)

    v_w = v_w + dw**2
    v_b = v_b + db**2

    w = w - (lr/(np.sqrt(v_w) + eps))*dw
    b = b - (lr/(np.sqrt(v_b) + eps))*db


  print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")

  return errors

list_error = adagrad()

plts(list_error)

def adam():
  w,b,lr,epochs= 2,-2,0.1,100
  v_w,v_b, eps  , beta=0,0 ,1e-8 , 0.9
  m_w,m_b = 0,0
  beta2=0.999
  errors = []
  print(X,Y)
  print(f"Error before updating weights: {error(w,b,X,Y)}")
  for i in range(epochs):
    dw,db = 0,0
    errors.append(error(w,b,X,Y))
    for x,y in zip(X,Y):
      dw+=grad_w(w,b,x,y)
      db+=grad_b(w,b,x,y)

    m_w = beta*v_w + (1-beta)*dw
    m_b = beta*v_b + (1-beta)*db


    v_w = beta2*v_w + (1-beta2)*dw**2
    v_b = beta2*v_b + (1-beta2)*db**2

    m_w_hat = m_w/(1-beta**(i+1))
    m_b_hat = m_b/(1-beta**(i+1))

    v_w_hat = v_w/(1-beta2**(i+1))
    v_b_hat = v_b/(1-beta2**(i+1))

    w = w - (lr/(np.sqrt(v_w_hat) + eps))*m_w_hat
    b = b - (lr/(np.sqrt(v_b_hat) + eps))*m_b_hat



  print(f"Error after updating weights: {error(w,b,X,Y)} weight : {w} bias : {b} ")

  return errors

list_error = RMS()

plts(list_error)
      
    </pre>

</body>
</html>
